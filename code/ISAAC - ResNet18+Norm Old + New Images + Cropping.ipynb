{"cells":[{"cell_type":"markdown","metadata":{"id":"YyHMSWSEik-u"},"source":["Completed Testing"]},{"cell_type":"markdown","metadata":{"id":"nMrdwLGdHnWv"},"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import sys\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51704,"status":"ok","timestamp":1733196412159,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"HnM_DEZcFjN-","outputId":"ed7972b3-9240-4164-d862-4c0cb4885a0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, random_split, Subset\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import sys\n","from google.colab import drive\n","import random\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1733196412159,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"i9rwBZn2feRA"},"outputs":[],"source":["# Define Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, sub_folder, transform, data_type='original'):\n","        self.root_dir = root_dir\n","        self.sub_folder = sub_folder\n","        self.transform = transform\n","        self.data_type = data_type\n","        self.image_paths = []\n","        self.labels = []\n","        self.sources = []\n","\n","        if self.data_type == 'original':\n","            self.load_original_data()\n","        elif self.data_type == 'augmentation':\n","            self.load_augmented_data()\n","\n","    # Iterate through the video folders\n","    def load_original_data(self):\n","        label_file = os.path.join(root_dir, 'old + new image frames.xlsx')\n","        numVideos = 0\n","        for video_folder in os.listdir(os.path.join(root_dir, sub_folder)):\n","            if os.path.isdir(os.path.join(root_dir, sub_folder, video_folder)):\n","                video_path = os.path.join(root_dir, sub_folder, video_folder)\n","                try:\n","                    labels_df = pd.read_excel(label_file, sheet_name=f'{video_folder}')\n","                    print(f\"Processing video folder: {video_folder}\")\n","                    numVideos += 1\n","                except ValueError:\n","                    # If the sheet does not exist, skip this folder and continue with the next\n","                    continue\n","\n","                # Iterate through image files and corresponding labels\n","                for img_filename in os.listdir(video_path):\n","                    if img_filename.endswith(\".jpg\"):\n","                        img_path = os.path.join(video_path, img_filename)\n","                        root, ext = os.path.splitext(img_filename)  # Split xxx_0.jpg into root and extension\n","                        frame_idx = int(root.split('_')[-1]) #splitting xxx_0 and storing 0 to frame_idx\n","                        labels = labels_df.loc[frame_idx, ['BAD QUALITY','CORD','FLUID']].values.astype('float32').squeeze()\n","                        source = labels_df.loc[frame_idx, 'SOURCE']\n","\n","                        self.image_paths.append(img_path)\n","                        self.labels.append(labels)\n","                        self.sources.append(source)\n","        print(f\"Number of videos: {numVideos}\")\n","\n","    def load_augmented_data(self):\n","        for video_folder in os.listdir(os.path.join(root_dir, sub_folder)):\n","            if os.path.isdir(os.path.join(root_dir, sub_folder, video_folder)):\n","                video_path = os.path.join(root_dir, sub_folder, video_folder)\n","                label_file = os.path.join(root_dir, 'Label',f'{video_folder}.xlsx')\n","                labels_df = pd.read_excel(label_file)\n","\n","                # Iterate through image files and corresponding labels\n","                for img_filename in os.listdir(video_path):\n","                    if img_filename.endswith(\".jpg\"):\n","                        img_path = os.path.join(video_path, img_filename)\n","                        root, ext = os.path.splitext(img_filename)  # Split 0.jpg_xxxxx.jpg into root and extension\n","                        labels = labels_df.loc[labels_df['FILENAME']==img_filename, ['BAD QUALITY','CORD','FLUID']].values.astype('float32').squeeze()\n","                        source = labels_df.loc[labels_df['FILENAME'] == img_filename, 'SOURCE'].values[0]\n","\n","                        self.image_paths.append(img_path)\n","                        self.labels.append(labels)\n","                        self.sources.append(source)\n","\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        labels = self.labels[idx]\n","        #source = self.sources[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        #return image, labels, source\n","        return image, labels\n","\n","\n","# Custom transform for vertical crop\n","class VerticalCrop:\n","    def __init__(self, crop_percentage=0.9):\n","        self.crop_percentage = crop_percentage\n","\n","    def __call__(self, img):\n","        width, height = img.size\n","        crop_height = int(height * self.crop_percentage)\n","        top = (height - crop_height) // 2\n","        bottom = top + crop_height\n","        return img.crop((0, top, width, bottom))\n","\n","# Custom transform for horizontal crop\n","class HorizontalCrop:\n","    def __init__(self, crop_percentage=0.9):\n","        self.crop_percentage = crop_percentage\n","\n","    def __call__(self, img):\n","        width, height = img.size\n","        crop_width = int(width * self.crop_percentage)\n","        left = (width - crop_width) // 2\n","        right = left + crop_width\n","        return img.crop((left, 0, right, height))\n","\n","# Function to calculate mean and standard deviation of the dataset\n","def calculate_mean_std(loader):\n","    # Placeholder for mean and standard deviation\n","    mean = 0.0\n","    std = 0.0\n","    total_images_count = 0\n","\n","    for images, *_ in loader:\n","        images = images.view(images.size(0), images.size(1), -1)\n","        mean += images.mean(2).sum(0)\n","        std += images.std(2).sum(0)\n","        total_images_count += images.size(0)\n","\n","    mean /= total_images_count\n","    std /= total_images_count\n","    return mean, std\n","\n","# Online calculation of mean and std with device management\n","def calculate_mean_std_online(data_loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Move initial total_mean and total_var tensors to the same device\n","    total_mean = torch.zeros(3, device=device)\n","    total_var = torch.zeros(3, device=device)\n","    total_images = 0\n","\n","    for images, *_ in data_loader:\n","        images = images.to(device)\n","        batch_samples = images.size(0)\n","        images = images.view(batch_samples, images.size(1), -1)\n","        batch_mean = images.mean(2).sum(0)\n","        batch_var = images.var(2).sum(0)\n","\n","        total_mean += batch_mean\n","        total_var += batch_var\n","        total_images += batch_samples\n","\n","    mean = total_mean / total_images\n","    std = torch.sqrt(total_var / total_images)\n","    return mean.cpu(), std.cpu()  # Return to CPU for further use if necessary\n","\n","def calculate_source_distribution(loader):\n","    # Access the original CustomDataset through the Subset\n","    dataset = loader.dataset.dataset  # This gets the original CustomDataset object\n","    source_counts = {0: 0, 1: 0}  # Assuming sources are labeled as 0 and 1\n","\n","    # Iterate through the indices in the dataset\n","    for idx in loader.dataset.indices:  # Subset loader keeps track of indices\n","        source = dataset.sources[idx]  # Access source using the indices from the original dataset\n","        source_counts[source] += 1\n","\n","    return source_counts\n","\n","# Function to split a dataset based on custom proportions\n","def custom_split(dataset, source_column, source_0_prop, source_1_prop):\n","    source_0_indices = [i for i, source in enumerate(dataset.sources) if source == 0]\n","    source_1_indices = [i for i, source in enumerate(dataset.sources) if source == 1]\n","\n","    def split_indices(indices, proportions):\n","        train_size = int(proportions[0] * len(indices))\n","        val_size = int(proportions[1] * len(indices))\n","        test_size = len(indices) - train_size - val_size\n","\n","        train_indices = indices[:train_size]\n","        val_indices = indices[train_size:train_size + val_size]\n","        test_indices = indices[train_size + val_size:]\n","        return train_indices, val_indices, test_indices\n","\n","    # Shuffle indices to ensure randomness\n","    random.shuffle(source_0_indices)\n","    random.shuffle(source_1_indices)\n","\n","    # Split the indices for each source\n","    train_0, val_0, test_0 = split_indices(source_0_indices, source_0_prop)\n","    train_1, val_1, test_1 = split_indices(source_1_indices, source_1_prop)\n","\n","    # Combine the indices from both sources\n","    train_indices = train_0 + train_1\n","    val_indices = val_0 + val_1\n","    test_indices = test_0 + test_1\n","\n","    # Shuffle combined indices for final datasets\n","    random.shuffle(train_indices)\n","    random.shuffle(val_indices)\n","    random.shuffle(test_indices)\n","\n","    return train_indices, val_indices, test_indices\n","\n","def initialize_data(root_dir, sub_folder, data_type='original'):\n","    transform_placeholder = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n","    full_dataset = CustomDataset(root_dir, sub_folder=sub_folder, transform=transform_placeholder, data_type=data_type)\n","\n","    # Define custom proportions\n","\n","    # Old Images\n","    source_0_props = [0.7, 0.1, 0.2]\n","\n","    # New Images\n","    source_1_props = [0.5, 0.1, 0.4]\n","\n","    # Get indices for custom splits\n","    train_indices, val_indices, test_indices = custom_split(\n","        full_dataset, source_column='SOURCE',\n","        source_0_prop=source_0_props,\n","        source_1_prop=source_1_props\n","    )\n","\n","    # Subset datasets using the indices\n","    train_subset = Subset(full_dataset, train_indices)\n","    val_subset = Subset(full_dataset, val_indices)\n","    test_subset = Subset(full_dataset, test_indices)\n","\n","    # Define transforms for normalization\n","    mean = torch.tensor([0.1012, 0.1017, 0.1015])\n","    std = torch.tensor([0.0992, 0.0994, 0.0979])\n","\n","    cropping_percent = 0.8\n","    train_transform = transforms.Compose([\n","        transforms.RandomChoice([\n","            transforms.Compose([\n","                transforms.CenterCrop((int(cropping_percent * 224), int(cropping_percent * 224))),\n","                transforms.Pad((12, 12, 12, 12), fill=0)\n","            ]),\n","            transforms.Compose([\n","                HorizontalCrop(crop_percentage=cropping_percent),\n","                transforms.Pad((12, 12, 12, 12), fill=0)\n","            ]),\n","            transforms.Compose([\n","                VerticalCrop(crop_percentage=cropping_percent),\n","                transforms.Pad((12, 12, 12, 12), fill=0)\n","            ]),\n","            transforms.Resize((224, 224))\n","        ]),\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n","    ])\n","\n","    val_test_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n","    ])\n","\n","    # Reapply transforms to subsets\n","    train_subset.dataset.transform = train_transform\n","    val_subset.dataset.transform = val_test_transform\n","    test_subset.dataset.transform = val_test_transform\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n","\n","    print()\n","    print('Data Size: train:', len(train_loader.dataset), 'val:', len(val_loader.dataset), 'test:', len(test_loader.dataset))\n","    print()\n","\n","    train_source_distribution = calculate_source_distribution(train_loader)\n","    val_source_distribution = calculate_source_distribution(val_loader)\n","    test_source_distribution = calculate_source_distribution(test_loader)\n","\n","    print(\"Source Distribution:\")\n","    print(\"Train Set:\", train_source_distribution)\n","    print(\"Validation Set:\", val_source_distribution)\n","    print(\"Test Set:\", test_source_distribution)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733196412159,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"DoO92OkMkjb7"},"outputs":[],"source":["# Define Model\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomResNet, self).__init__()\n","        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(num_features, num_classes),\n","            nn.Sigmoid()  # Sigmoid activation for multi-label classification\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"qEbhuhoIlAKE"},"outputs":[],"source":["# Train Model\n","def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs):\n","    global num_classes\n","    global device\n","    train_losses = []\n","    train_accuracies = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        # Initialize loss and accuracy variables for this epoch\n","        running_loss = 0.0\n","        correct_predictions = 0\n","\n","        # Initialize the progress bar\n","        train_progress_bar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}', position=0,leave=True)\n","\n","        #for images, labels, _ in train_progress_bar:\n","        for images, labels in train_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            predicts = (outputs \u003e 0.5).float()\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss = loss.item()\n","            running_loss += train_loss * images.size(0)\n","            correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            train_progress_bar.set_postfix({'Batch Loss': train_loss, 'Batch Accuracy': acc})\n","            torch.save(model, \"saved_model.pt\")\n","\n","        # Calculate average loss and accuracy for the epoch\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_predictions / len(train_loader.dataset)\n","        train_losses.append(epoch_loss)\n","        train_accuracies.append(epoch_accuracy)\n","        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n","\n","        # Save the state of the model\n","        torch.save(model.state_dict(), \"saved_model_state.pt\")\n","        torch.save(model, \"saved_model.pt\")\n","\n","        ######################### Validation loop\n","        print('Validation:')\n","        model.eval()\n","        val_running_loss = 0.0\n","        val_correct_predictions = 0\n","\n","        # Initialize the progress bar for validation\n","        val_progress_bar = tqdm(val_loader, total=len(val_loader), desc=f'Epoch {epoch + 1}', position=0,leave=True)\n","\n","        with torch.no_grad():\n","            #for images, labels, _ in val_progress_bar:\n","            for images, labels in val_progress_bar:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                predicts = (outputs \u003e 0.5).float()\n","                acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss = loss.item()\n","                val_running_loss += val_loss * images.size(0)\n","                val_correct_predictions += acc * images.size(0)\n","\n","                # Update the progress bar with the loss and accuracy\n","                val_progress_bar.set_postfix({'Batch Loss': val_loss, 'Batch Accuracy': acc})\n","        # Calculate average loss and accuracy for the validation\n","        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","        val_epoch_accuracy = val_correct_predictions / len(val_loader.dataset)\n","        val_losses.append(val_epoch_loss)\n","        val_accuracies.append(val_epoch_accuracy)\n","        print(f'Validation Epoch {epoch+1}/{num_epochs} - Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_accuracy:.4f}')\n","        print('------------------------------------------------------------------------------------------------')\n","\n","    return train_losses, train_accuracies, val_losses, val_accuracies\n","\n","def plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies):\n","    # Plotting the training and validation loss\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title('Epoch vs Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    # Plotting the training and validation accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label='Train Accuracy')\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.title('Epoch vs Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"pwXm7_n692Wk"},"outputs":[],"source":["# Load Model\n","def load_model(model_path):\n","    # Load the saved model\n","    model = torch.load(model_path)\n","    model.eval()  # Set the model to evaluation mode\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"tgV8syztlXPM"},"outputs":[],"source":["# Test Model\n","def test_model(test_loader, model_path, criterion):\n","    global num_classes\n","    global device\n","    # Initialize variables to store predictions and true labels\n","\n","    # Load the model\n","    model = load_model(model_path)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    # Test loop\n","    model.eval()  # Set the model to evaluation mode\n","    test_running_loss = 0.0\n","    test_correct_predictions = 0\n","\n","    # Initialize the progress bar for testing\n","    test_progress_bar = tqdm(test_loader, total=len(test_loader), desc='Testing', position=0, leave=True)\n","\n","    misclassified_images = []\n","\n","    with torch.no_grad():\n","        image_index = 0\n","        #for images, labels, _ in test_progress_bar:\n","        for images, labels in test_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predicts = (outputs \u003e 0.5).float()\n","\n","            #Check for misclassifications\n","            for idx, (pred, true) in enumerate(zip(predicts, labels)):\n","                if not torch.equal(pred, true):\n","                    misclassified_images.append({\n","                        'image_index': image_index + idx,\n","                        'predicted_labels': pred.cpu().numpy(),\n","                        'true_labels': true.cpu().numpy()\n","                    })\n","\n","            # ... [rest of your existing code in the loop] ...\n","            # Store predictions and true labels for later metrics calculation\n","            all_preds.extend(predicts.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss = loss.item()\n","            test_running_loss += test_loss * images.size(0)\n","            test_correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            test_progress_bar.set_postfix({'Batch Loss': test_loss, 'Batch Accuracy': acc})\n","\n","            image_index += images.size(0)\n","\n","    # Calculate average loss and accuracy for the test set\n","    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n","    test_epoch_accuracy = test_correct_predictions / len(test_loader.dataset)\n","\n","    # Calculate other metrics\n","    precision = precision_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    recall = recall_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    f1 = f1_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    roc_auc = roc_auc_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    tn, fp, fn, tp = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten()).ravel()\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(f'Test Metrics:')\n","    print(f'{\"-\"*50}')\n","    print(f'Loss      : {test_epoch_loss:.4f}')\n","    print(f'Accuracy  : {test_epoch_accuracy:.4f}')\n","    print(f'Precision : {precision:.4f}')\n","    print(f'Recall    : {recall:.4f}')\n","    print(f'True Negatives : {tn}')\n","    print(f'False Positives: {fp}')\n","    print(f'False Negatives: {fn}')\n","    print(f'True Positives : {tp}')\n","    print(f'F1 Score  : {f1:.4f}')\n","    print(f'ROC AUC   : {roc_auc:.4f}')\n","    print(\"=\"*50)\n","\n","    # Initialize variables to store class-wise metrics\n","    class_precisions = []\n","    class_recalls = []\n","    class_f1s = []\n","    class_roc_aucs = []\n","\n","    # Calculate metrics for each class\n","    num_classes = np.array(all_labels).shape[1]  # Assuming all_labels is a 2D array\n","    for i in range(num_classes):\n","        y_true = np.array(all_labels)[:, i]\n","        y_pred = np.array(all_preds)[:, i]\n","\n","        precision = precision_score(y_true, y_pred, zero_division=0)\n","        recall = recall_score(y_true, y_pred, zero_division=0)\n","        f1 = f1_score(y_true, y_pred, zero_division=0)\n","        roc_auc = roc_auc_score(y_true, y_pred)\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","        class_precisions.append(precision)\n","        class_recalls.append(recall)\n","        class_f1s.append(f1)\n","        class_roc_aucs.append(roc_auc)\n","\n","        print(f\"Metrics for class {i}:\")\n","        print(f\"  Precision : {precision:.4f}\")\n","        print(f\"  Recall    : {recall:.4f}\")\n","        print(f\"  F1 Score  : {f1:.4f}\")\n","        print(f\"  ROC AUC   : {roc_auc:.4f}\")\n","        print(f'True Negatives : {tn}')\n","        print(f'False Positives: {fp}')\n","        print(f'False Negatives: {fn}')\n","        print(f'True Positives : {tp}')\n","        print(\"-\"*20)\n","\n","    # If you want a summary report\n","    print(\"Summary Classification Report:\")\n","    print(classification_report(np.array(all_labels), np.array(all_preds), zero_division=0))\n","\n","    # Print or return the misclassified images\n","    print(f\"Total Misclassified Images: {len(misclassified_images)}\")\n","    for misclassified in misclassified_images:\n","        print(f\"Image Index: {misclassified['image_index']}, Predicted Labels: {misclassified['predicted_labels']}, True Labels: {misclassified['true_labels']}\")\n","\n","    return all_labels, all_preds, misclassified_images"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"gwRSRZdnlmG7"},"outputs":[],"source":["# Main function to run the whole pipeline\n","def main(root_dir, sub_folder, model, criterion, optimizer, num_epochs, data_type='original'):\n","    # Initialize Data\n","    train_loader, val_loader, test_loader = initialize_data(root_dir, sub_folder, data_type=data_type)\n","    #check_loaders(train_loader, val_loader, test_loader)\n","\n","    '''global device\n","    model = CustomResNet(num_classes).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)'''\n","\n","    # Train Model\n","    train_losses, train_accuracies, val_losses, val_accuracies = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=num_epochs)\n","    plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies)\n","\n","    # Test Model\n","    all_labels, all_preds = test_model(test_loader, model, criterion)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"OMnkZMAso4k_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def show_images(images, labels=None, num_images=4):\n","    fig, axs = plt.subplots(1, num_images, figsize=(15, 3))\n","    for i in range(num_images):\n","        axs[i].imshow(images[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n","        axs[i].axis('off')\n","        if labels is not None:\n","            axs[i].set_title(labels[i])\n","\n","def check_loaders(train_loader, val_loader, test_loader):\n","    print(\"Train Loader: {} batches ({} images)\".format(len(train_loader), len(train_loader.dataset)))\n","    print(\"Validation Loader: {} batches ({} images)\".format(len(val_loader), len(val_loader.dataset)))\n","    print(\"Test Loader: {} batches ({} images)\".format(len(test_loader), len(test_loader.dataset)))\n","\n","    # Optionally, visualize some images from each loader\n","    #for images, labels, _ in train_loader:\n","    for images, labels in train_loader:\n","        show_images(images)\n","        break  # Just show the first batch"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733196412292,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"Cgv5wEuDnK51"},"outputs":[],"source":["#train-valid-test\n","def tvt(train_loader, val_loader, test_loader, model, criterion, optimizer, num_epochs):\n","\n","    # Train Model\n","    train_losses, train_accuracies, val_losses, val_accuracies = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=num_epochs)\n","    plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies)\n","\n","    # Test Model\n","    all_labels, all_preds = test_model(test_loader, model, criterion)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72794,"status":"ok","timestamp":1733196485082,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"kLWSjH6rna2Z","outputId":"910d9966-70a3-4441-c382-2ada92f67e7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing video folder: 01.09.31 hrs __0025097\n","Processing video folder: 01.09.47 hrs __0025098\n","Processing video folder: AM12 (Converted)\n","Processing video folder: BM12 (Converted)\n","Processing video folder: LP SONO 001 POST FNL CONUS\n","Processing video folder: LP SONO 001 POST FST CONUS\n","Processing video folder: LPPILOTLAT1\n","Processing video folder: LPPILOTLAT2\n","Processing video folder: LPPILOTLAT3\n","Processing video folder: LPPILOTSITTING2\n","Processing video folder: US00000L\n","Processing video folder: XM12 (Converted)\n","Processing video folder: YM12 (Converted)\n","Processing video folder: 200106061016030017SMP_crop\n","Processing video folder: 200106061016340018SMP_crop\n","Processing video folder: 200106061016530019SMP_crop\n","Processing video folder: 200106061017230020SMP_crop\n","Processing video folder: 202402091355160039ABD_crop\n","Processing video folder: 202402091355230040ABD_crop\n","Processing video folder: 202402091355350041ABD_crop\n","Processing video folder: 202402091355470042ABD_crop\n","Processing video folder: 202402091355560043ABD_crop\n","Processing video folder: 202402091356080044ABD_cropCONUS\n","Processing video folder: 202402091356160045ABD_crop\n","Processing video folder: 202402091356410046ABD_crop\n","Processing video folder: 202402091356500047ABD_crop\n","Processing video folder: 202402091356590048ABD_crop\n","Number of videos: 27\n","\n","Data Size: train: 2713 val: 407 test: 951\n","\n","Source Distribution:\n","Train Set: {0: 2373, 1: 340}\n","Validation Set: {0: 339, 1: 68}\n","Test Set: {0: 678, 1: 273}\n"]}],"source":["# Initialize Data\n","root_dir = '/content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files'    # CHANGE BASED ON FOLDER LOCATION\n","sub_folder = 'Old + New Video Frames'\n","num_classes=3\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","org_train_loader, org_val_loader, org_test_loader = initialize_data(root_dir,\n","                                                                    sub_folder,\n","                                                                    data_type='original')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1733196485082,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"y-wwFYORpYEK"},"outputs":[],"source":["from typing import Optional, Sequence\n","\n","import torch\n","from torch import Tensor\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"88Sbsz-wqHeq"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00\u003c00:00, 158MB/s]\n","Epoch 1: 100%|██████████| 85/85 [43:33\u003c00:00, 30.74s/it, Batch Loss=0.136, Batch Accuracy=0.947]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20 - Loss: 0.1601, Accuracy: 0.9368\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 13/13 [05:31\u003c00:00, 25.50s/it, Batch Loss=0.686, Batch Accuracy=0.855]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 1/20 - Loss: 0.2685, Accuracy: 0.9099\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 85/85 [12:07\u003c00:00,  8.55s/it, Batch Loss=0.0995, Batch Accuracy=0.96]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/20 - Loss: 0.0734, Accuracy: 0.9736\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 13/13 [00:38\u003c00:00,  2.99s/it, Batch Loss=0.298, Batch Accuracy=0.942]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 2/20 - Loss: 0.2455, Accuracy: 0.9304\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 85/85 [12:10\u003c00:00,  8.59s/it, Batch Loss=0.143, Batch Accuracy=0.933]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/20 - Loss: 0.0610, Accuracy: 0.9801\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 13/13 [00:39\u003c00:00,  3.08s/it, Batch Loss=0.171, Batch Accuracy=0.957]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 3/20 - Loss: 0.0560, Accuracy: 0.9787\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 85/85 [12:06\u003c00:00,  8.55s/it, Batch Loss=0.0661, Batch Accuracy=0.96]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/20 - Loss: 0.0367, Accuracy: 0.9862\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 13/13 [00:39\u003c00:00,  3.02s/it, Batch Loss=0.0711, Batch Accuracy=0.971]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 4/20 - Loss: 0.1157, Accuracy: 0.9623\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 85/85 [12:00\u003c00:00,  8.48s/it, Batch Loss=0.0174, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/20 - Loss: 0.0369, Accuracy: 0.9856\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 13/13 [00:39\u003c00:00,  3.01s/it, Batch Loss=0.914, Batch Accuracy=0.841]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 5/20 - Loss: 1.0436, Accuracy: 0.7928\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 85/85 [12:06\u003c00:00,  8.55s/it, Batch Loss=0.0325, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/20 - Loss: 0.0378, Accuracy: 0.9870\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 13/13 [00:38\u003c00:00,  3.00s/it, Batch Loss=0.23, Batch Accuracy=0.942]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 6/20 - Loss: 0.0732, Accuracy: 0.9705\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 85/85 [12:01\u003c00:00,  8.48s/it, Batch Loss=0.0482, Batch Accuracy=0.973]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/20 - Loss: 0.0328, Accuracy: 0.9899\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 13/13 [00:36\u003c00:00,  2.82s/it, Batch Loss=0.262, Batch Accuracy=0.942]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 7/20 - Loss: 0.0949, Accuracy: 0.9689\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 85/85 [12:13\u003c00:00,  8.63s/it, Batch Loss=0.0316, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/20 - Loss: 0.0391, Accuracy: 0.9866\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 13/13 [00:38\u003c00:00,  2.99s/it, Batch Loss=0.16, Batch Accuracy=0.913]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 8/20 - Loss: 0.0873, Accuracy: 0.9689\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 85/85 [12:05\u003c00:00,  8.54s/it, Batch Loss=0.0016, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/20 - Loss: 0.0242, Accuracy: 0.9921\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 13/13 [00:39\u003c00:00,  3.02s/it, Batch Loss=0.214, Batch Accuracy=0.942]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 9/20 - Loss: 0.0625, Accuracy: 0.9836\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 85/85 [12:05\u003c00:00,  8.53s/it, Batch Loss=0.00179, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/20 - Loss: 0.0193, Accuracy: 0.9924\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 13/13 [00:39\u003c00:00,  3.00s/it, Batch Loss=0.0638, Batch Accuracy=0.957]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 10/20 - Loss: 0.0472, Accuracy: 0.9853\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11: 100%|██████████| 85/85 [12:09\u003c00:00,  8.58s/it, Batch Loss=0.0111, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/20 - Loss: 0.0144, Accuracy: 0.9951\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11: 100%|██████████| 13/13 [00:37\u003c00:00,  2.87s/it, Batch Loss=0.183, Batch Accuracy=0.957]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 11/20 - Loss: 0.0762, Accuracy: 0.9754\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12: 100%|██████████| 85/85 [11:58\u003c00:00,  8.46s/it, Batch Loss=0.0106, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/20 - Loss: 0.0155, Accuracy: 0.9957\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12: 100%|██████████| 13/13 [00:39\u003c00:00,  3.00s/it, Batch Loss=0.0766, Batch Accuracy=0.942]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 12/20 - Loss: 0.0418, Accuracy: 0.9877\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13: 100%|██████████| 85/85 [11:58\u003c00:00,  8.46s/it, Batch Loss=0.000917, Batch Accuracy=1]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/20 - Loss: 0.0108, Accuracy: 0.9967\n","Validation:\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13: 100%|██████████| 13/13 [00:37\u003c00:00,  2.90s/it, Batch Loss=0.075, Batch Accuracy=0.957]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Epoch 13/20 - Loss: 0.0679, Accuracy: 0.9844\n","------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14:  67%|██████▋   | 57/85 [07:59\u003c03:53,  8.34s/it, Batch Loss=0.0524, Batch Accuracy=0.99]"]}],"source":["if __name__ == \"__main__\":\n","    num_classes=3\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Initialize Model, Loss, and Optimizer\n","    model = CustomResNet(num_classes).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Train and validate model\n","    tvt(org_train_loader, org_val_loader, org_test_loader, model, criterion, optimizer, num_epochs=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0rj7ZJI9gpy"},"outputs":[],"source":["criterion = nn.BCELoss()\n","all_labels, all_preds, misclassified_images = test_model(org_test_loader, \"saved_model.pt\", criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RC2U8ofo-f5p"},"outputs":[],"source":["# Test Model\n","def test_miss_model(test_loader, model_path, criterion):\n","    global num_classes\n","    global device\n","    # Initialize variables to store predictions and true labels\n","\n","    # Load the model\n","    model = load_model(model_path)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    # Test loop\n","    model.eval()  # Set the model to evaluation mode\n","    test_running_loss = 0.0\n","    test_correct_predictions = 0\n","\n","    # Initialize the progress bar for testing\n","    test_progress_bar = tqdm(test_loader, total=len(test_loader), desc='Testing', position=0, leave=True)\n","\n","    misclassified_images = []\n","\n","    with torch.no_grad():\n","        image_index = 0\n","        #for images, labels, _ in test_progress_bar:\n","        for images, labels in test_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predicts = (outputs \u003e 0.5).float()\n","\n","            # Check for misclassifications\n","            for idx, (img, pred, true) in enumerate(zip(images, predicts, labels)):\n","                if not torch.equal(pred, true):\n","                    misclassified_images.append({\n","                        'image_index': image_index + idx,  # Store the index\n","                        'image': img.cpu(),  # Store the image tensor\n","                        'predicted_labels': pred.cpu().numpy(),\n","                        'true_labels': true.cpu().numpy()\n","                    })\n","\n","            # ... [rest of your existing code in the loop] ...\n","            # Store predictions and true labels for later metrics calculation\n","            all_preds.extend(predicts.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss = loss.item()\n","            test_running_loss += test_loss * images.size(0)\n","            test_correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            test_progress_bar.set_postfix({'Batch Loss': test_loss, 'Batch Accuracy': acc})\n","\n","            image_index += images.size(0)\n","\n","    # Calculate average loss and accuracy for the test set\n","    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n","    test_epoch_accuracy = test_correct_predictions / len(test_loader.dataset)\n","\n","    # Calculate other metrics\n","    precision = precision_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    recall = recall_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    f1 = f1_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    roc_auc = roc_auc_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    tn, fp, fn, tp = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten()).ravel()\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(f'Test Metrics:')\n","    print(f'{\"-\"*50}')\n","    print(f'Loss      : {test_epoch_loss:.4f}')\n","    print(f'Accuracy  : {test_epoch_accuracy:.4f}')\n","    print(f'Precision : {precision:.4f}')\n","    print(f'Recall    : {recall:.4f}')\n","    print(f'True Negatives : {tn}')\n","    print(f'False Positives: {fp}')\n","    print(f'False Negatives: {fn}')\n","    print(f'True Positives : {tp}')\n","    print(f'F1 Score  : {f1:.4f}')\n","    print(f'ROC AUC   : {roc_auc:.4f}')\n","    print(\"=\"*50)\n","\n","    # Initialize variables to store class-wise metrics\n","    class_precisions = []\n","    class_recalls = []\n","    class_f1s = []\n","    class_roc_aucs = []\n","\n","    # Calculate metrics for each class\n","    num_classes = np.array(all_labels).shape[1]  # Assuming all_labels is a 2D array\n","    for i in range(num_classes):\n","        y_true = np.array(all_labels)[:, i]\n","        y_pred = np.array(all_preds)[:, i]\n","\n","        precision = precision_score(y_true, y_pred, zero_division=0)\n","        recall = recall_score(y_true, y_pred, zero_division=0)\n","        f1 = f1_score(y_true, y_pred, zero_division=0)\n","        roc_auc = roc_auc_score(y_true, y_pred)\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","        class_precisions.append(precision)\n","        class_recalls.append(recall)\n","        class_f1s.append(f1)\n","        class_roc_aucs.append(roc_auc)\n","\n","        print(f\"Metrics for class {i}:\")\n","        print(f\"  Precision : {precision:.4f}\")\n","        print(f\"  Recall    : {recall:.4f}\")\n","        print(f\"  F1 Score  : {f1:.4f}\")\n","        print(f\"  ROC AUC   : {roc_auc:.4f}\")\n","        print(f'True Negatives : {tn}')\n","        print(f'False Positives: {fp}')\n","        print(f'False Negatives: {fn}')\n","        print(f'True Positives : {tp}')\n","        print(\"-\"*20)\n","\n","    # If you want a summary report\n","    print(\"Summary Classification Report:\")\n","    print(classification_report(np.array(all_labels), np.array(all_preds), zero_division=0))\n","\n","    # Print or return the misclassified images\n","    print(f\"Total Misclassified Images: {len(misclassified_images)}\")\n","    for misclassified in misclassified_images:\n","        print(f\"Image Index: {misclassified['image_index']}, Predicted Labels: {misclassified['predicted_labels']}, True Labels: {misclassified['true_labels']}\")\n","\n","    return all_labels, all_preds, misclassified_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWoh5VADDawk"},"outputs":[],"source":["criterion = nn.BCELoss()\n","all_labels, all_preds, misclassified_images = test_miss_model(org_test_loader, \"saved_model.pt\", criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7Z0Yglphfzy"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","from tqdm import tqdm\n","\n","# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Dataset for Inference\n","class InferenceDataset(Dataset):\n","    def __init__(self, root_dir, transform):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        # Collecting image paths with a debug message\n","        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith(\".JPG\")]\n","        if not self.image_paths:\n","            print(f\"No images found in {root_dir}. Please check the directory path and file extensions.\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        #labels = self.labels[idx]\n","        #source = self.sources[idx]  # Ensure self.sources is populated\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, img_path\n","\n","# Function to Generate Predictions\n","def generate_predictions(model_path, inference_folder):\n","    # Load the saved model and move it to the device\n","    model = torch.load(model_path, map_location=device)\n","    model.eval()  # Set the model to evaluation mode\n","    model.to(device)\n","\n","    # Define image transformations\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=torch.tensor([0.1006, 0.1011, 0.1009]), std=torch.tensor([0.0998, 0.0999, 0.0985]))\n","    ])\n","\n","    # Initialize dataset and dataloader\n","    inference_dataset = InferenceDataset(root_dir=inference_folder, transform=transform)\n","    inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=False)\n","\n","    predictions = []\n","    with torch.no_grad():\n","        for images, img_paths in tqdm(inference_loader, desc='Generating Predictions'):\n","            images = images.to(device)\n","            outputs = model(images)\n","\n","            # Convert predictions to binary values\n","            preds = (outputs \u003e 0.5).float().cpu().numpy()\n","\n","            # Collect predictions and corresponding image paths\n","            for img_path, pred in zip(img_paths, preds):\n","                predictions.append({\n","                    \"image_path\": img_path,\n","                    \"predicted_labels\": pred\n","                })\n","\n","    if not predictions:\n","        print(\"No predictions generated. Please check if the dataset is loading images correctly.\")\n","    return predictions\n","\n","# Folder containing new images for inference\n","inference_folder = \"/content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict\"\n","model_path = \"saved_model.pt\"  # Path to your saved model\n","\n","# Generate predictions\n","predictions = generate_predictions(model_path, inference_folder)\n","\n","print()\n","\n","# Display predictions\n","for pred in predictions:\n","    file_name = pred['image_path'].split(\"Predict/\")[1]\n","    print(\"Image:\", file_name, f\"Predicted Labels: {pred['predicted_labels']}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyTGf3ZxFjnK"},"outputs":[],"source":["#function for all missclassified images\n","\n","import matplotlib.pyplot as plt\n","import math\n","\n","def show_misclassified_images(misclassified_images):\n","    num_images = len(misclassified_images)\n","    num_cols = 5  # You can adjust the number of columns\n","    num_rows = math.ceil(num_images / num_cols)\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","\n","    for i in range(num_images):\n","        img_info = misclassified_images[i]\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","        index = img_info['image_index']  # Get the image index\n","\n","        ax = axs[i // num_cols, i % num_cols]  # Determine row and column position\n","        ax.imshow(img)\n","        ax.set_title(f\"Index: {index}\\nPred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for i in range(num_images, num_rows * num_cols):\n","        axs[i // num_cols, i % num_cols].axis('off')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2oc8WjRE8Na"},"outputs":[],"source":["# Now display the misclassified images\n","show_misclassified_images(misclassified_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgImz_nfFpLy"},"outputs":[],"source":["#calling all misclassified images\n","\n","show_misclassified_images(misclassified_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCSPJVRwGCl8"},"outputs":[],"source":["def calculate_fp_fn(misclassified_images, num_classes):\n","    # Initialize counters\n","    false_positives = [0] * num_classes\n","    false_negatives = [0] * num_classes\n","\n","    # Analyze each misclassified image\n","    for img_info in misclassified_images:\n","        predicted_labels = img_info['predicted_labels']\n","        true_labels = img_info['true_labels']\n","\n","        for i in range(num_classes):\n","            # False positive: predicted is 1, true is 0\n","            if predicted_labels[i] == 1 and true_labels[i] == 0:\n","                false_positives[i] += 1\n","            # False negative: predicted is 0, true is 1\n","            elif predicted_labels[i] == 0 and true_labels[i] == 1:\n","                false_negatives[i] += 1\n","\n","    return false_positives, false_negatives\n","\n","# Call this function after you have the misclassified_images list\n","false_positives, false_negatives = calculate_fp_fn(misclassified_images, 3)\n","\n","# Print the results\n","print(\"False Positives per Class:\", false_positives)\n","print(\"False Negatives per Class:\", false_negatives)\n"]},{"cell_type":"markdown","metadata":{"id":"22dXPi2dIUqM"},"source":["False Positives per Class: [11, 0, 0]\n","\n","For the first class (let's call it Class 1), your model incorrectly predicted the positive label (predicted as '1') 11 times when the true label was negative (actual '0'). This means there were 11 instances where the model thought the feature corresponding to Class 1 was present, but it actually wasn't.\n","For the second and third classes (Class 2 and Class 3), there were no false positives. This means the model never incorrectly predicted the presence of these features; it did not mistakenly identify these features in any of the images where they were actually absent.\n","False Negatives per Class: [0, 3, 10]\n","\n","For Class 1, there were no false negatives. This means that whenever the feature corresponding to Class 1 was present in an image, the model always identified it correctly.\n","For Class 2, there were 3 instances where the model failed to identify the presence of the feature (predicted as '0') when it was actually present (true '1'). In other words, the model missed this feature 3 times when it should have detected it.\n","For Class 3, the situation is more pronounced with 10 false negatives. This indicates that the model frequently failed to recognize the presence of the feature corresponding to Class 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7WpWtnOGzhX"},"outputs":[],"source":["def filter_specific_misclassifications(misclassified_images):\n","    filtered_images = []\n","    for img_info in misclassified_images:\n","        true_labels = img_info['true_labels']\n","        predicted_labels = img_info['predicted_labels']\n","        # Check if Class 1 is '1' and Class 2 \u0026 3 are '0' in true labels\n","        # and if Class 1 is correctly predicted as '1'\n","        if true_labels[0] == 1 and true_labels[1] == 0 and true_labels[2] == 0 and predicted_labels[0] == 1:\n","            filtered_images.append(img_info)\n","    return filtered_images\n","\n","specific_misclassifications = filter_specific_misclassifications(misclassified_images)\n","\n","def show_filtered_images(filtered_images, num_cols=5):\n","    num_images = len(filtered_images)\n","    if num_images == 0:\n","        print(\"No images meet the specified criteria.\")\n","        return\n","\n","    num_rows = math.ceil(num_images / num_cols)\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","\n","    for i, img_info in enumerate(filtered_images):\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","\n","        # Handle the case when axs is 1-dimensional\n","        if num_images \u003c= num_cols:\n","            ax = axs[i]  # Index as a 1D array\n","        else:\n","            ax = axs[i // num_cols, i % num_cols]  # Index as a 2D array\n","\n","        ax.imshow(img)\n","        ax.set_title(f\"Pred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for j in range(i + 1, num_rows * num_cols):\n","        if num_images \u003c= num_cols:\n","            axs[j].axis('off')  # Index as a 1D array\n","        else:\n","            axs[j // num_cols, j % num_cols].axis('off')  # Index as a 2D array\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFMl7Q2cLGCI"},"outputs":[],"source":["# Filter and show the specific misclassifications\n","show_filtered_images(specific_misclassifications)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TafL0ROiLLvd"},"outputs":[],"source":["def filter_specific_predictions(misclassified_images):\n","    filtered_images = []\n","    for img_info in misclassified_images:\n","        predicted_labels = img_info['predicted_labels']\n","        # Check if predicted Class 1 is '1' and Classes 2 \u0026 3 are '0'\n","        if predicted_labels[0] == 1 and predicted_labels[1] == 0 and predicted_labels[2] == 0:\n","            filtered_images.append(img_info)\n","    return filtered_images\n","\n","specific_predictions = filter_specific_predictions(misclassified_images)\n","\n","import matplotlib.pyplot as plt\n","import math\n","\n","def show_filtered_images(filtered_images, num_cols=5):\n","    num_images = len(filtered_images)\n","    if num_images == 0:\n","        print(\"No images meet the specified criteria.\")\n","        return\n","\n","    num_rows = math.ceil(num_images / num_cols)\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","    # If there's only one row, axs will be 1D, so we need to handle this case\n","    if num_rows == 1:\n","        axs = [axs]  # Make axs a list of lists\n","\n","    for i, img_info in enumerate(filtered_images):\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","        ax = axs[i // num_cols][i % num_cols] # Access the correct element now that axs is 2D\n","        ax.imshow(img)\n","        ax.set_title(f\"Pred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for j in range(i + 1, num_rows * num_cols):\n","        axs[j // num_cols][j % num_cols].axis('off')  # Access correctly\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_CkQFfMMcCh"},"outputs":[],"source":["# Filter and show the specific predictions\n","show_filtered_images(specific_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDvzORfxiuoJ"},"outputs":[],"source":["# Save model to google drive\n","\n","model_to_save = torch.load(\"saved_model.pt\")\n","\n","# Specify the path in your Google Drive\n","drive_path = '/content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Code Files'\n","os.makedirs(drive_path, exist_ok=True)\n","\n","# Save the model to Google Drive\n","model_path = os.path.join(drive_path, \"ResNet18_Best_Model.pth\")\n","torch.save(model_to_save.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","provenance":[{"file_id":"1uKzPiAb0dbGPF5JJ19lyiSXZbuLghwq5","timestamp":1727644489319}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}